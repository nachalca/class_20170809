---
title: "Hierarchical logistic regression models in Stan"
author: ""
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
source('../helper_funs.R')
```

```{r load-packages, message=FALSE, warning=FALSE}
library("rstanarm")
library("ggplot2")
library("bayesplot")
library("dplyr")
```

```{r rstan-options}
options(mc.cores = parallel::detectCores())
```


### Load the data

```{r logistic-regression-data}
cps <- readRDS('cps_2016.RDS')
pstrat <- readRDS('frame.RDS')
```

Let's fit the following model in rstanarm

$$
y_n \sim \text{Binomial}(K_n, p_n) \\
\text{logit}(p_n) = \alpha + \alpha^\text{age}_{j[n]} +
\alpha^\text{state}_{k[n]} +
X_n \times \beta\\
\alpha^\text{age}_j = \sigma_{\text{age}}\times \eta^\text{age}_j \\
\alpha^\text{state}_k = \sigma_{\text{state}} \times \eta^\text{state}_k \\
\eta^\text{age}_j, \eta^\text{state}_k \sim \text{Normal}(0, 1) \\
\pi \sim \text{Dirichlet}(\nu) \\
\sigma_\text{global} \sim \text{exponential}(1) \\
\vec{\sigma} = \sqrt{c \times \pi \times \sigma_\text{global}^2} \\
\sigma_\text{age} = \vec{\sigma}_{[1]} \\
\sigma_\text{state} = \vec{\sigma}_{[2]}
$$

This is a fairly complex model, takes about 80 lines to code this up in Stan code.
We can do it in a few lines in rstanarm. What's more, we don't have to keep track
of the differences between the poststratification frame and the fitted data.

```{r rstanarm-prep}
cps %>%
  group_by(
    age,state
  ) %>%
  summarise(
    vote = sum(vote),
    K = sum(N),
    did_not_vote = K - vote,
    state_pres_vote = first(state_pres_vote)
  ) %>%
  mutate(
    age_pred = as.integer(age)
  ) -> cps_rstanarm
pstrat %>%
  group_by(
    age, state
  ) %>%
  summarise(
    cell_elig = sum(N_elig),
    state_pres_vote = first(state_pres_vote)
  )  %>%
  mutate(
    age_pred = as.integer(age)
  ) -> pstrat_rstanarm
```

## Fit the model

Rstanarm models are specified using `lmer4` syntax.

```{r model-fit}
fit1 <-
  stan_glmer(
    cbind(vote, did_not_vote) ~ 1 +
    state_pres_vote + age_pred + (1 | age) +
      (1 | state),
    data = cps_rstanarm,
    iter = 1000,
    adapt_delta = 0.8,
    family = binomial(link = "logit"),
    QR = TRUE
  )
```

### Using shinystan

```{r shinystan-query, eval=FALSE}
library("shinystan")
ss_mod <- as.shinystan(fit1)
launch_shinystan(ss_mod)
```

###Posterior predictive checks

```{r pp-query}
pp_fit1 <- posterior_predict(fit1, newdata = fit1$data) 

state_sel_vec <- fit1$data$state %in% c('HI','ME','OH')
ppc_stat_grouped(y = (fit1$data$vote / fit1$data$K)[state_sel_vec], 
                 yrep = sweep(pp_fit1[,state_sel_vec], 2, STATS = fit1$data$K[state_sel_vec], FUN = '/'),
                 group = fit1$data$state[state_sel_vec], stat = min)
```

## Postrat 

```{r poststrat}
ps_draws <- 
  posterior_linpred(
    fit1,
    transform = TRUE,
    newdata = pstrat_rstanarm
  )

dim(ps_draws)

state_intervals <-
  intervals_of_interest(
    ps = pstrat_rstanarm,
    group_facs = 'state',
    cell_counts = 'cell_elig',
    ps_reps = ps_draws,
    probs = c(0.05, 0.25, 0.5, 0.75, 0.95)
  )
    
age_intervals <-
  intervals_of_interest(
    ps = pstrat_rstanarm,
    group_facs = 'age',
    cell_counts = 'cell_elig',
    ps_reps = ps_draws,
    probs = c(0.05, 0.25, 0.5, 0.75, 0.95)
  )
```

### Poststrat interval plots
We can plot our intervals 2 ways. The first is with the intervals that we have calculated.

```{r intervals-plot-manual}
theme_set(bayesplot::theme_default())
n_ints <- nrow(state_intervals$intervals)
overall_median <- median(state_intervals$intervals[,'50%'])
state_ints <-
  state_intervals$intervals %>%
  arrange(
    `95%`
  ) %>% 
  mutate(
    y = seq(n_ints, 1, by = -1)
  )
state_names <- state_ints$group
y <- state_ints$y

p1 <- ggplot(state_ints) +
  geom_segment(aes(x = `5%`, xend = `95%`, y = y, yend = y), colour = '#005b96') +
  geom_segment(aes(x = `25%`, xend = `75%`, y = y, yend=y),size = 2, colour = '#03396c') +
  geom_point(aes(x = `50%`, y = y)) +
  scale_y_continuous(breaks = y, labels = state_names) +
  xlab(NULL) + ylab(NULL) +
  geom_vline(xintercept = overall_median)
p1
```

We can also use `bayesplot` to make our interval plots

```{r mcmc-intervals}
df <- data.frame(age_intervals$dists)
names(df) <- names(age_intervals$dists)

mcmc_intervals(df, point_est = "none")
```

###Hierarchical scale priors


One thing to keep in mind when fitting models is that
defining the hierarchy of the scale parameters of the
random intercepts can improve sampling, decrease 
posterior intervals, and improve out-of-sample
forecasts.

Sophie (Yajuan) Si, Jonah Gabry, Andrew Gelman, and I have
been working on a prior for hierarhcical scale parameters
in models where we have random intercepts that are two-way
and three-way interactions between main categories.

```{r rstanarm-prep-interaction,eval=FALSE, include=FALSE}
cps %>%
  group_by(
    age,state,edu
  ) %>%
  summarise(
    vote = sum(vote),
    K = sum(N),
    did_not_vote = K - vote,
    state_pres_vote = first(state_pres_vote)
  ) %>%
  mutate(
    age_pred = as.integer(age)
  ) -> cps_rstanarm
pstrat %>%
  group_by(
    age, state, edu
  ) %>%
  summarise(
    cell_elig = sum(N_elig),
    state_pres_vote = first(state_pres_vote)
  )  %>%
  mutate(
    age_pred = as.integer(age)
  ) -> pstrat_rstanarm
```

For instance, if we wanted to fit the following model:

```{r model-fit-interact,eval=FALSE}
fit2 <-
  stan_glmer(
    cbind(vote, did_not_vote) ~ 1 +
    state_pres_vote + age_pred + (1 | age) +
      (1 | state) + (1 | edu) + (1 | age:state) +
      (1 | age:edu) + (1 | state:edu),
    data = cps_rstanarm,
    iter = 1000,
    adapt_delta = 0.8,
    family = binomial(link = "logit"),
    QR = TRUE
  )
```

rstanarm would automatically treat the prior for the scale parameters for
`age:state`, `age:edu` and `state:edu` the same as it would `state` and `age`.
To review, the default prior for rstanarm is to use:

$$
\pi \sim \text{Dirichlet}(\nu) \\
\sigma_\text{global} \sim \text{exponential}(1) \\
\vec{\sigma} = c \times \pi \times \sigma_\text{global}^2
$$

But our prior should probably reflect that if `age` isn't an informative 
grouping for our data, then `age:state` probably won't be useful either. 
We can encode that intuition by making the parameter $\sigma_\text{age:state}
\propto \sigma_\text{age} \times \sigma_\text{state}$

We might also imagine learning a separate scale parameter for all of the K-way
interactions, along with a global scale, akin to the parameterization of rstanarm's
prior over scale parameters. The full spec is below:

$$
\sigma_\text{global} \sim \text{Student_t}^+(\nu, 0, \text{scale}) \\
\lambda_k^{(1)} \sim \text{Normal}^+(0, 1) \\
\lambda_k^{(l)} = \delta^{(l)} \prod_{l_0 \in M^{(k)}} \lambda_{l_0}^{(1)} \\
\delta^{(l)} \sim \text{Normal}^+(0, 1)
$$
The priors for the random intercepts in the model above would become:

$$
\alpha^\text{age}_j = \sigma_\text{global} \times \, \lambda^{(1)}_1 \times \eta^\text{age}_j \\
\alpha^\text{state}_j = \sigma_\text{global} \times \, \lambda^{(1)}_2 \times \eta^\text{state}_j \\
\alpha^\text{edu}_j = \sigma_\text{global} \times \, \lambda^{(1)}_3 \times \eta^\text{edu}_j \\
\alpha^{\text{age:state}}_j = \sigma_\text{global} \times \, \lambda^{(2)}_1 \times \eta^{\text{age:state}}_j \\
\alpha^{\text{age:edu}}_j = \sigma_\text{global} \times \, \lambda^{(2)}_2 \times \eta^{\text{age:edu}}_j \\
\alpha^{\text{state:age}}_j = \sigma_\text{global} \times \, \lambda^{(2)}_3 \times \eta^{\text{state:age}}_j \\
\eta^\text{age}_j, \eta^\text{state}_j, \eta^\text{edu}_j, \eta^\text{age:state}_j, \eta^\text{age:edu}_j, \eta^\text{state:edu}_j \sim \text{Normal}(0, 1) \\
$$

If you'd like to try this prior, run the following code to install the 
a branch of `rstanarm` called `structured_prior`

```{r struct-prior, eval = FALSE}
if (!require(devtools)) {
  install.packages("devtools")
  library(devtools)
}
install_github("stan-dev/rstanarm", ref = "structured_prior_merge", args = "--preclean", build_vignettes = FALSE)
```

The model with the prior above can be run like so:

```{r model-fit-struct-prior, eval = FALSE}
fit_struct <-
  stan_glmer(
    cbind(vote, did_not_vote) ~ 1 +
    state_pres_vote + age_pred + (1 | age) +
      (1 | state) + (1 | edu) + (1 | age:state) +
      (1 | age:edu) + (1 | state:edu),
    data = cps_rstanarm,
    iter = 1000,
    prior_covariance = 
     mrp_structured(
       group_level_scale = 1, 
       group_level_df = 7
     ), 
    adapt_delta = 0.9,
    family = binomial(link = "logit"),
    QR = TRUE
  )
```

We have a paper that'll be on arXiV soon with results using this prior vs. 
the half-normal priors.