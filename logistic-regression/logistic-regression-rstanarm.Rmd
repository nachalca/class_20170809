---
title: "Hierarchical logistic regression models in Stan"
author: ""
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
source('../helper_funs.R')
```

```{r load-packages, message=FALSE, warning=FALSE}
library("rstanarm")
library("ggplot2")
library("bayesplot")
library("dplyr")
```

```{r rstan-options}
options(mc.cores = parallel::detectCores())
```


### Load the data

```{r logistic-regression-data}
cps <- readRDS('cps_2016.RDS')
pstrat <- readRDS('frame.RDS')
```

Let's fit the following model in rstanarm

$$
y_n \sim \text{Binomial}(K_n, p_n) \\
\text{logit}(p_n) = \alpha + \alpha^\text{age}_{j[n]} +
\alpha^\text{state}_{k[n]} +
X_n \times \beta\\
\alpha^\text{age}_j = \sigma_{\alpha^\text{age}} \times \eta^\text{age}_j \\
\alpha^\text{state}_k = \sigma_{\alpha^\text{state}} \times \eta^\text{state}_k \\
\alpha^\text{age}_j, \alpha^\text{state}_k \sim \text{Normal}(0, 1) \\
\pi \sim \text{Dirichlet}(\nu) \\
\sigma_\text{global} \sim \text{exponential}(1) \\
\vec{\sigma} = \sqrt{c \times \pi \times \sigma_\text{global}^2} \\
\sigma_\text{age} = \vec{\sigma}_{[1]} \\
\sigma_\text{state} = \vec{\sigma}_{[2]}
$$

This is a fairly complex model, takes about 80 lines to code this up in Stan code.
We can do it in a few lines in rstanarm. What's more, we don't have to keep track
of the differences between the poststratification frame and the fitted data.

```{r rstanarm-prep}
cps %>%
  group_by(
    age,state
  ) %>%
  summarise(
    vote = sum(vote),
    K = sum(N),
    did_not_vote = K - vote,
    state_pres_vote = first(state_pres_vote)
  ) %>%
  mutate(
    age_pred = as.integer(age)
  ) -> cps_rstanarm
pstrat %>%
  group_by(
    age, state
  ) %>%
  summarise(
    cell_elig = sum(N_elig),
    state_pres_vote = first(state_pres_vote)
  )  %>%
  mutate(
    age_pred = as.integer(age)
  ) -> pstrat_rstanarm
```

## Fit the model

Rstanarm models are specified using `lmer4` syntax.

```{r model-fit}
fit1 <-
  stan_glmer(
    cbind(vote, did_not_vote) ~ 1 +
    state_pres_vote + age_pred + (1 | age) +
      (1 | state),
    data = cps_rstanarm,
    iter = 1000,
    adapt_delta = 0.8,
    family = binomial(link = "logit"),
    QR = TRUE
  )
```

### Using shinystan

```{r shinystan-query, eval=FALSE}
library("shinystan")
ss_mod <- as.shinystan(fit1)
launch_shinystan(ss_mod)
```

###Posterior predictive checks

```{r pp-query}
pp_fit1 <- posterior_predict(fit1, newdata = fit1$data) 

state_sel_vec <- fit1$data$state %in% c('HI','ME','OH')
ppc_stat_grouped(y = (fit1$data$vote / fit1$data$K)[state_sel_vec], 
                 yrep = sweep(pp_fit1[,state_sel_vec], 2, STATS = fit1$data$K[state_sel_vec], FUN = '/'),
                 group = fit1$data$state[state_sel_vec], stat = min)
```

## Postrat 

```{r poststrat}
ps_draws <- 
  posterior_linpred(
    fit1,
    transform = TRUE,
    newdata = pstrat_rstanarm
  )

dim(ps_draws)

state_intervals <-
  intervals_of_interest(
    ps = pstrat_rstanarm,
    group_facs = 'state',
    cell_counts = 'cell_elig',
    ps_reps = ps_draws,
    probs = c(0.1, 0.5, 0.9)
  )
    
age_intervals <-
  intervals_of_interest(
    ps = pstrat_rstanarm,
    group_facs = 'age',
    cell_counts = 'cell_elig',
    ps_reps = ps_draws,
    probs = c(0.1, 0.5, 0.9)
  )
```

### Priors for random intercept interactions

One thing to keep in mind when fitting models is that
defining the hierarchy of the scale parameters of the
random intercepts can improve sampling, decrease 
posterior intervals, and improve out-of-sample
forecasts.

Sophie (Yajuan) Si, Jonah Gabry, Andrew Gelman, and I have
been working on a prior for hierarhcical scale parameters
in models where we have random intercepts that are two-way
and three-way interactions between main categories.

```{r rstanarm-prep-interaction,eval=FALSE, include=FALSE}
cps %>%
  group_by(
    age,state,edu
  ) %>%
  summarise(
    vote = sum(vote),
    K = sum(N),
    did_not_vote = K - vote,
    state_pres_vote = first(state_pres_vote)
  ) %>%
  mutate(
    age_pred = as.integer(age)
  ) -> cps_rstanarm
pstrat %>%
  group_by(
    age, state, edu
  ) %>%
  summarise(
    cell_elig = sum(N_elig),
    state_pres_vote = first(state_pres_vote)
  )  %>%
  mutate(
    age_pred = as.integer(age)
  ) -> pstrat_rstanarm
```

For instance, if we wanted to fit the following model:

```{r model-fit-interact,eval=FALSE}
fit2 <-
  stan_glmer(
    cbind(vote, did_not_vote) ~ 1 +
    state_pres_vote + age_pred + (1 | age) +
      (1 | state) + (1 | edu) + (1 | age:state) +
      (1 | age:edu) + (1 | state:edu),
    data = cps_rstanarm,
    iter = 1000,
    adapt_delta = 0.8,
    family = binomial(link = "logit"),
    QR = TRUE
  )
```

rstanarm would automatically treat the prior for the scale parameters for
`age:state`, `age:edu` and `state:edu` the same as it would `state` and `age`.
To review, the default prior for rstanarm is to use:

$$
\pi \sim \text{Dirichlet}(\nu) \\
\sigma_\text{global} \sim \text{exponential}(1) \\
\vec{\sigma} = c \times \pi \times \sigma_\text{global}^2
$$

But our prior should probably reflect that if `age` isn't an informative 
grouping for our data, then `age:state` probably won't be useful either. 
We can encode that intuition by making the parameter $\sigma_\text{age:state}
\propto \sigma_\text{age} \times \sigma_\text{state}$

We might also imagine learning a separate scale parameter for all of the K-way
interactions, along with a global scale, akin to the parameterization of rstanarm's
prior over scale parameters. The full spec is below:

$$
\sigma_\text{global} \sim \text{Student_t}^+(\nu, 0, \text{scale}) \\
\lambda_k^{(1)} \sim \text{Normal}^+(0, 1) \\
\lambda_k^{(l)} = \delta^{(l)} \prod_{l_0 \in M^{(k)}} \lambda_{l_0}^{(1)} \\
\delta^{(l)} \sim \text{Normal}^+(0, 1) \\
\tau^{(l)}_k = \sigma_\text{global} \times \, \lambda^{(l)}_k\\
$$

If you'd like to try this prior, run the following code to install the 
a branch of `rstanarm` called `structured_prior`

```{r struct-prior, eval = FALSE}
if (!require(devtools)) {
  install.packages("devtools")
  library(devtools)
}
install_github("stan-dev/rstanarm", ref = "structured_prior_merge", args = "--preclean", build_vignettes = FALSE)
```

The model with the prior above can be run like so:

```{r model-fit-struct-prior, eval = FALSE}
fit_struct <-
  stan_glmer(
    cbind(vote, did_not_vote) ~ 1 +
    state_pres_vote + age_pred + (1 | age) +
      (1 | state) + (1 | edu) + (1 | age:state) +
      (1 | age:edu) + (1 | state:edu),
    data = cps_rstanarm,
    iter = 1000,
    prior_covariance = 
     mrp_structured(
       group_level_scale = 1, 
       group_level_df = 7
     ), 
    adapt_delta = 0.9,
    family = binomial(link = "logit"),
    QR = TRUE
  )
```

We have a paper that'll be on arXiV soon with results using this prior vs. 
the half-normal priors.