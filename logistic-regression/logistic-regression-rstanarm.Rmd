---
title: "Hierarchical logistic regression models in Stan"
author: ""
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
source('../helper_funs.R')
```

```{r load-packages, message=FALSE, warning=FALSE}
library("rstanarm")
library("ggplot2")
library("bayesplot")
library("dplyr")
```

```{r rstan-options}
options(mc.cores = parallel::detectCores())
```


### Load the data

```{r logistic-regression-data}
cps <- readRDS('cps_2016.RDS')
pstrat <- readRDS('frame.RDS')
```

Let's fit the following model in rstanarm

$$
y_n \sim \text{Binomial}(K_n, p_n) \\
\text{logit}(p_n) = \alpha + \alpha^\text{age}_{j[n]} +
\alpha^\text{state}_{k[n]} +
X_n \times \beta\\
\alpha^\text{age}_j = \sigma_{\alpha^\text{age}} \times \eta^\text{age}_j \\
\alpha^\text{state}_k = \sigma_{\alpha^\text{state}} \times \eta^\text{state}_k \\
\alpha^\text{age}_j, \alpha^\text{state}_k \sim \text{Normal}(0, 1) \\
\pi \sim \text{Dirichlet}(\nu) \\
\sigma_\text{global} \sim \text{exponential}(1) \\
\vec{\sigma} = \sqrt{c \times \pi \times \sigma_\text{global}^2} \\
\sigma_\text{age} = \vec{\sigma}_{[1]} \\
\sigma_\text{state} = \vec{\sigma}_{[2]}
$$

This is a fairly complex model, takes about 80 lines to code this up in Stan code.
We can do it in a few lines in rstanarm. What's more, we don't have to keep track
of the differences between the poststratification frame and the fitted data.

```{r rstanarm-prep}
cps %>%
  group_by(
    age,state
  ) %>%
  summarise(
    vote = sum(vote),
    K = sum(N),
    did_not_vote = K - vote,
    state_pres_vote = first(state_pres_vote)
  ) %>%
  mutate(
    age_pred = as.integer(age)
  ) -> cps_rstanarm
pstrat %>%
  group_by(
    age, state
  ) %>%
  summarise(
    cell_elig = sum(N_elig),
    state_pres_vote = first(state_pres_vote)
  )  %>%
  mutate(
    age_pred = as.integer(age)
  ) -> pstrat_rstanarm
```

## Fit the model

Rstanarm models are specified using \texttt{lmer4} syntax.

```{r model-fit}
fit1 <-
  stan_glmer(
    cbind(vote, did_not_vote) ~ 1 +
    state_pres_vote + age_pred + (1 | age) +
      (1 | state),
    data = cps_rstanarm,
    iter = 1000,
    adapt_delta = 0.8,
    family = binomial(link = "logit")
  )
```

## Postrat 

```{r poststrat}
ps_draws <- 
  posterior_linpred(
    fit1,
    transform = TRUE,
    newdata = pstrat_rstanarm
  )

dim(ps_draws)

state_intervals <-
  intervals_of_interest(
    ps = pstrat_rstanarm,
    group_facs = 'state',
    cell_counts = 'cell_elig',
    ps_reps = ps_draws,
    probs = c(0.1, 0.5, 0.9)
  )
    
age_intervals <-
  intervals_of_interest(
    ps = pstrat_rstanarm,
    group_facs = 'age',
    cell_counts = 'cell_elig',
    ps_reps = ps_draws,
    probs = c(0.1, 0.5, 0.9)
  )
```

### Priors for random intercept interactions

One thing to keep in mind when fitting models is that
defining the hierarchy of the scale parameters of the
random intercepts can improve sampling, decrease 
posterior intervals, and improve out-of-sample
forecasts.

Sophie (Yajuan) Si, Jonah Gabry, Andrew Gelman, and I have
been working on a prior for hierarhcical scale parameters
in models where we have random intercepts that are two-way
and three-way interactions between main categories.

For instance, if we wanted to fit the following model:

```{r model-fit}
fit2 <-
  stan_glmer(
    cbind(vote, did_not_vote) ~ 1 +
    state_pres_vote + age_pred + (1 | age) +
      (1 | state) + (1 age:state),
    data = cps_rstanarm,
    iter = 1000,
    adapt_delta = 0.8,
    family = binomial(link = "logit")
  )
```

rstanarm would automatically treat the prior for the scale parameter for \texttt{age:state}
the same as it would \texttt{state} and \texttt{age}. To review, the default prior for
rstanarm is to use:

$$
\pi \sim \text{Dirichlet}(\nu) \\
\sigma_\text{global} \sim \text{exponential}(1) \\
\vec{\sigma} = c \times \pi \times \sigma_\text{global}^2
$$

But our prior should probably reflect that if \texttt{age} isn't an informative 
grouping for our data, then \texttt{age:state} probably won't be useful either. 
We can encode that intuition by making the parameter $\sigma_\text{age:state}
\propto \sigma_\text{age} \times \sigma_\text{state}$

We might also imagine learning a separate scale parameter for all of the K-way
interactions, along with a global scale, akin to the parameterization of rstanarm's
prior over scale parameters. The full spec is below:

$$
\sigma_\text{global} \sim \text{Normal}^+(0, 1) \\
\lambda_k^{(1)} \sim \text{Normal}^+(0, 1) \\
\lambda_k^{(l)} = \delta^{(l)} \prod_{l_0 \in M^{(k)}} \lambda_{l_0}^{(1)} \\
\delta^{(l)} \sim \text{Normal}^+(0, 1)
$$

If you'd like to try this prior, run the following code to install the 
a branch of \texttt{rstanarm} called \texttt{structured\_prior}

```{r struct-prior, eval = FALSE}
if (!require(devtools)) {
  install.packages("devtools")
  library(devtools)
}
install_github("stan-dev/rstanarm", ref = "structured_prior_merge", args = "--preclean", build_vignettes = FALSE)
```

We have a paper that'll be on arXiV soon with results using this prior vs. 
the half-normal priors.